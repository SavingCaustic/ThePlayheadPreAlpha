TODO
====

* Audio in.. Kanske ska vi ha en AudioManager. Denna kan ha in-buffers (alltid 2st), så kopplar DSP upp sig till deessa.
  Och det är audioManager som slår av/på själva drivern..

* WavReader kan ha Intro + SideA + SideB. När läsning av sideB börjar ska sideA uppdateras, om inte då hela alltet ryms i
intro + A + B. 32+32+32 = 96 kB. => mono 2 sek. Sedan behöver läsning ske varje sek.
Denna är ju synkron. Bara en tråd.

* ugh.. Hur göra med allt detta med Rack, units. Untis som kanske används någon annanstans.
Och hur göra med MidiCC som kan användas av både synth och effekt. Viktigt att mapping finns på rack.
cc_mapping, tja har man params har man cc_mapping. Vet iofs inte om rack i sig har params, bara settings..

* rensa i skapande av globala objekt.

* device.json ska läsas direkt och styra uppkoppling (kan även innehålla debug.)
Så rent generellt, en json fil innehåller "settings" som blir en unsorted_map i ett objekt.
Vart går settings i device? Globalt objekt.

* kunna spara parametrar som patch.
klura på logik för att ha både synth- och rack-patches.

* kunna ladda parametrar som patch.

* snap på parametrar. Stöd för filter-typ i DummySynth.
Då behöver vi väl jämföra nuv position med ny föreslagen, så vi inte kallar lambdan i onödan.
Och då behöver vi veta om värdet vi är på väg att skriva är persistant eller temp (automation).
Kan vänta med automation med förbreda för det. (på väg att skapa egen klass här..)

* mjuka övergångar - delta för parametrar. Registreras vid compile-time.

* skicka svar från parametrar med både 0-1 och värdet, ex "40L"
gången är ju FE=>WS=>PE=>rack=>module. Ev tänka DNS här, så module=>WS istället för hela vägen tillbaka..

* Kunna skicka disableRack så att vi kan göra operationer som ladda synth m.m.

* Flera egenskaper *rack* sätts från endpoint. Risk för race.

* metonom - var ska den in?

* modul-kul: delay.

* modul-kul: ocatver.

* kodstäd

* börja med timing..



FIXAT:

* Midi-in hotplugging. At interval (Stu)

* bättre initiering av callback - varför krash? Är det midi eller audio?

* Kötta i midi-implementeringen. Separat lab.

* Rensa så midi inte använder char

* Flytta endpoints till egen fil

* sine-synth. mono och bara AR, men för att testa LUT, frekvens o midi.

* döp om klassnamn för syntar men kanske behåll filnamn så man ser i vsCode.

* Ta bort depenencien till Rack på Unit. Pekar direkt på buffern bara.

* nåt som behöver göras om med parametrar. värdet ska ju omskapas innan det kastas in i lambdan.
Lambdan sja bara göra halva jobbet - skicka till moduler.
Sen behöver värdet sparas och skickas tillbaka. Gör utanför (return)
på sikt *kanske* vi kan skicka hela lambdan till ngn 'smoother' som körs 24ppqn typ.
eller tja, det kanske är så att automatiseringen får sköta det. en smoother per automatisering..

* Rensa dummyModel från grejer som borde vara i interface

* nåt sätt att aktivera / avaktivera rack för uppspelning. (inte lita på is_null)
* flytta rack så de är på stacken istället (snabbare buffer)

* byta huvudobject till stackspace

* rackInFocus i playerEngine(?) så den vet vart midi-meddelanden ska skickas.
rackReceivingMidi = 0-n; (-1 no midi in)

* midi-mapping av controller<>parameeter.. 
hur skulle detta gå till? Börja först med att testa cc..
asset-file per synth. funkar tills vidare. Sen vill man kanske kunna styra effekter också via CC..

* skapa message buffer för dessa svar

* websocket-server som kan ta emot dessa svar och skicka vidare till klient
  * Få igång websocket mellan FE och BE.
  * Kunna skicka nån parameter. Vad? NoteOn typ.. format?
  {cmd:noteOn,key:69}, {cmd:noteOff:key:69}
  {cmd:setParam,unit:synth,name:cutoff,value:100}

* testa audioError när parameter sätts..
* fixa lateny i audioError..
